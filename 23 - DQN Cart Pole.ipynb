{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_size = len(env.observation_space.sample())\n",
    "K = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network model for Deep Q Learning\n",
    "def NNModel(input_shape, K, hidden_layer_sizes,act):\n",
    "    X_input=Input(input_shape)\n",
    "    #First Layer\n",
    "    X = Dense(hidden_layer_sizes[0], input_shape=(state_size,), activation=act, kernel_initializer='he_uniform')(X_input)\n",
    "    if len(hidden_layer_sizes)>1:\n",
    "        for M in hidden_layer_sizes[1:]:\n",
    "            X = Dense(M, activation=act, kernel_initializer='he_uniform')(X)\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(K, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole_DQN_model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\", hidden_layer_sizes=[512,256,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(model,s, eps):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - max(eps,0.05)):\n",
    "        return np.argmax(model.predict(s))\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "DQNModel= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\",hidden_layer_sizes=[200,200,200])\n",
    "GAMMA=0.95\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_episodes = 500\n",
    "scores=[]\n",
    "for it in range(n_episodes):\n",
    "    # begin a new episode\n",
    "    state=env.reset()\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    done=False\n",
    "    score=0\n",
    "    while not done:\n",
    "#        epsilon=0.97**(it)\n",
    "#        time.sleep(0.05)\n",
    "        env.render()\n",
    "        Q_values = DQNModel.predict(state)\n",
    "        action = epsilon_greedy(DQNModel,state, eps=epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if not done or score == env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        nextQ_values = DQNModel.predict(next_state)\n",
    "        target=Q_values\n",
    "        #Update the target values for the action chosen\n",
    "        target[0][action] = reward + GAMMA * np.max(nextQ_values)               \n",
    "        # update the weights\n",
    "        DQNModel.fit(state, target, batch_size=1, verbose=0)\n",
    "        # update state\n",
    "        state = next_state\n",
    "        score=score+1\n",
    "        if done:                   \n",
    "            print(\"episode: {}, score: {}\".format(it, score))\n",
    "            scores.append(score)\n",
    "DQNModel.save('DQNPoleNoReplayNoTarget')\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(model):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    s = np.reshape(s, [1, state_size])\n",
    "    while not done:\n",
    "        a = epsilon_greedy(model,s,eps=0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        s = np.reshape(s, [1, state_size])\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        episode_reward += r\n",
    "    print(\"Episode reward:\", episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tf.keras.models.load_model('DQNPoleNoReplayNoTarget')\n",
    "for i in range(10):\n",
    "    watch_agent(Model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-learning is inherently unstable, your q values improve first than might abruptly get worse. This is due to the fact that the target values are non-stationary, and neural networks are very sensitive to overfitting and will try to catch this non-stationary and trajectory dependent target. To deal with this issue, we implement two main approaches: <br> \n",
    "-Experience replay: For instance, we put 10 thousand transitions into a buffer and sample a mini-batch of samples of size 64 from this buffer to train the deep network. This forms an input dataset which is stable enough for training. As we randomly sample from the replay buffer, the data is more independent of each other and closer to i.i.d. <br>\n",
    "-Target network: We create two deep networks. We use the first one to retrieve Q values while the second one includes all updates in the training. After say 20 episodes, we synchronize the weights of two networks. The purpose is to fix the Q-value targets temporarily so we donâ€™t have a moving target to chase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "d = deque(maxlen=3)\n",
    "for i in range(1, 7):\n",
    "    d.append(i)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Replay Experince and Batch update no target network memory 2000\n",
    "memory = deque(maxlen=2000)\n",
    "min_experiences=100\n",
    "batch_sz=64\n",
    "np.random.seed(1)\n",
    "state_size = env.observation_space.shape[0]\n",
    "DQNModel= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\",hidden_layer_sizes=[200,200,200])\n",
    "GAMMA=0.95\n",
    "n_episodes = 500\n",
    "scores=[]\n",
    "for it in range(n_episodes):\n",
    "    # begin a new episode\n",
    "    state=env.reset()\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    done=False\n",
    "    score = 0\n",
    "    best_score=0\n",
    "    epsilon=0.97**(it)\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel,state, eps=epsilon)\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if not done or score == env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        #Stochastic Gradient is generally unstable to fit DQN in RL\n",
    "        #Instead we will use minibatch and experince replay idea\n",
    "        #Experince replay will alleviate the problem of non-stationarity as well\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experiences:\n",
    "            # randomly select a batch\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_sz))\n",
    "            states = np.zeros((batch_sz, state_size))\n",
    "            next_states = np.zeros((batch_sz, state_size))\n",
    "            actions, rewards, dones = [], [], []\n",
    "\n",
    "            for i in range(batch_sz):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append(minibatch[i][1])\n",
    "                rewards.append(minibatch[i][2])\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append(minibatch[i][4])\n",
    "            \n",
    "            targets = DQNModel.predict(states)\n",
    "            target_nexts = DQNModel.predict(next_states)\n",
    "           \n",
    "            for i in range(batch_sz):\n",
    "                # correction on the Q value for the action used\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                #Targets are computed using the target network which is not updated all the time\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))   \n",
    "                                \n",
    "            # update the weights of the current network\n",
    "            DQNModel.fit(states, targets, batch_size=batch_sz, verbose=0)\n",
    "        # update state\n",
    "        state = next_state\n",
    "        score=score+1\n",
    "        if done:                   \n",
    "            print(\"episode: {}, score: {}\".format(it, score))\n",
    "    scores.append(score)\n",
    "DQNModel.save('DQNPoleBatchNoTarget')\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tf.keras.models.load_model('DQNPoleBatchNoTarget')\n",
    "for i in range(10):\n",
    "    watch_agent(Model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With replay and batch update and with Target Network memory 10000 update every 20\n",
    "memory = deque(maxlen=10000)\n",
    "min_experiences=1000\n",
    "batch_sz=64\n",
    "np.random.seed(1)\n",
    "DQNModel= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\",hidden_layer_sizes=[200,200,200])\n",
    "#We will also create a copy of the network which we don't update very frequently, this will also resolve\n",
    "#the unstability issue observed in DQNs\n",
    "TargetNetwork= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\", hidden_layer_sizes=[200,200,200])\n",
    "TargetNetwork.set_weights(DQNModel.get_weights()) \n",
    "GAMMA=0.95\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_episodes = 500\n",
    "scores=[]\n",
    "for it in range(n_episodes):\n",
    "    # begin a new episode\n",
    "    state=env.reset()\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    done=False\n",
    "    score = 0\n",
    "    best_score=0\n",
    "    epsilon=0.97**(it)\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel,state, eps=epsilon)\n",
    "#        time.sleep(0.05)\n",
    "#        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if not done or score == env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        #Stochastic Gradient is generally unstable to fit DQN in RL\n",
    "        #Instead we will use minibatch and experince replay idea\n",
    "        #Experince replay will alleviate the problem of non-stationarity as well\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experiences:\n",
    "            # randomly select a batch\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_sz))\n",
    "            states = np.zeros((batch_sz, state_size))\n",
    "            next_states = np.zeros((batch_sz, state_size))\n",
    "            actions, rewards, dones = [], [], []\n",
    "\n",
    "            for i in range(batch_sz):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append(minibatch[i][1])\n",
    "                rewards.append(minibatch[i][2])\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append(minibatch[i][4])\n",
    "            \n",
    "            targets = DQNModel.predict(states)\n",
    "            target_nexts = TargetNetwork.predict(next_states)\n",
    "           \n",
    "            for i in range(batch_sz):\n",
    "                # correction on the Q value for the action used\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                #Targets are computed using the target network which is not updated all the time\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))   \n",
    "                                \n",
    "            # update the weights of the current network\n",
    "            DQNModel.fit(states, targets, batch_size=batch_sz, verbose=0)\n",
    "        # update state\n",
    "        state = next_state\n",
    "        score=score+1\n",
    "        if done:                   \n",
    "            print(\"episode: {}, score: {}\".format(it, score))\n",
    "    scores.append(score)\n",
    "    if (it+1)%20 == 0:\n",
    "        TargetNetwork.set_weights(DQNModel.get_weights()) \n",
    "TargetNetwork.save('DQNPoleBatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tf.keras.models.load_model('DQNPoleBatch')\n",
    "for i in range(20):\n",
    "    watch_agent(Model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With replay and batch update and with Target Network memory 10000 update every 50\n",
    "memory = deque(maxlen=10000)\n",
    "min_experiences=1000\n",
    "batch_sz=64\n",
    "np.random.seed(1)\n",
    "DQNModel= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\", hidden_layer_sizes=[200,200,200])\n",
    "#We will also create a copy of the network which we don't update very frequently, this will also resolve\n",
    "#the unstability issue observed in DQNs\n",
    "TargetNetwork= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"relu\", hidden_layer_sizes=[200,200,200])\n",
    "TargetNetwork.set_weights(DQNModel.get_weights()) \n",
    "GAMMA=0.95\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_episodes = 500\n",
    "scores=[]\n",
    "for it in range(n_episodes):\n",
    "    # begin a new episode\n",
    "    state=env.reset()\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    done=False\n",
    "    score = 0\n",
    "    best_score=0\n",
    "    epsilon=0.97**(it)\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel,state, eps=epsilon)\n",
    "#        time.sleep(0.05)\n",
    "#        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if not done or score == env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        #Stochastic Gradient is generally unstable to fit DQN in RL\n",
    "        #Instead we will use minibatch and experince replay idea\n",
    "        #Experince replay will alleviate the problem of non-stationarity as well\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experiences:\n",
    "            # randomly select a batch\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_sz))\n",
    "            states = np.zeros((batch_sz, state_size))\n",
    "            next_states = np.zeros((batch_sz, state_size))\n",
    "            actions, rewards, dones = [], [], []\n",
    "\n",
    "            for i in range(batch_sz):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append(minibatch[i][1])\n",
    "                rewards.append(minibatch[i][2])\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append(minibatch[i][4])\n",
    "            \n",
    "            targets = DQNModel.predict(states)\n",
    "            target_nexts = TargetNetwork.predict(next_states)\n",
    "           \n",
    "            for i in range(batch_sz):\n",
    "                # correction on the Q value for the action used\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                #Targets are computed using the target network which is not updated all the time\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))   \n",
    "                                \n",
    "            # update the weights of the current network\n",
    "            DQNModel.fit(states, targets, batch_size=batch_sz, verbose=0)\n",
    "        # update state\n",
    "        state = next_state\n",
    "        score=score+1\n",
    "        if done:                   \n",
    "            print(\"episode: {}, score: {}\".format(it, score))\n",
    "    scores.append(score)\n",
    "    if (it+1)%50 == 0:\n",
    "        TargetNetwork.set_weights(DQNModel.get_weights()) \n",
    "TargetNetwork.save('DQNPoleBatchTargetUpdate50')\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tf.keras.models.load_model('DQNPoleBatchTargetUpdate50')\n",
    "for i in range(5):\n",
    "    watch_agent(Model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With replay and batch update and with Target Network memory soft update tanh activation\n",
    "memory = deque(maxlen=10000)\n",
    "min_experiences=1000\n",
    "batch_sz=64\n",
    "TAU=0.05\n",
    "np.random.seed(1)\n",
    "DQNModel= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"tanh\", hidden_layer_sizes=[200,200,200])\n",
    "TargetNetwork= NNModel(input_shape=(state_size,),K=env.action_space.n, act=\"tanh\", hidden_layer_sizes=[200,200,200])\n",
    "GAMMA=0.95\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_episodes = 500\n",
    "scores=[]\n",
    "for it in range(n_episodes):\n",
    "    # begin a new episode\n",
    "    state=env.reset()\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    done=False\n",
    "    score = 0\n",
    "    best_score=0\n",
    "    epsilon=0.97**(it)\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel,state, eps=epsilon)\n",
    "#        time.sleep(0.05)\n",
    "#        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if not done or score == env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        #Stochastic Gradient is generally unstable to fit DQN in RL\n",
    "        #Instead we will use minibatch and experince replay idea\n",
    "        #Experince replay will alleviate the problem of non-stationarity as well\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experiences:\n",
    "            # randomly select a batch\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_sz))\n",
    "            states = np.zeros((batch_sz, state_size))\n",
    "            next_states = np.zeros((batch_sz, state_size))\n",
    "            actions, rewards, dones = [], [], []\n",
    "\n",
    "            for i in range(batch_sz):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append(minibatch[i][1])\n",
    "                rewards.append(minibatch[i][2])\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append(minibatch[i][4])\n",
    "            \n",
    "            targets = DQNModel.predict(states)\n",
    "            target_nexts = TargetNetwork.predict(next_states)\n",
    "           \n",
    "            for i in range(batch_sz):\n",
    "                # correction on the Q value for the action used\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                #Targets are computed using the target network which is only soft updated\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))   \n",
    "                                \n",
    "            # update the weights of the current network\n",
    "            DQNModel.fit(states, targets, batch_size=batch_sz, verbose=0)\n",
    "            \n",
    "            #Soft Update the Target Network\n",
    "            q_model_theta = DQNModel.get_weights()\n",
    "            target_model_theta = TargetNetwork.get_weights()\n",
    "            \n",
    "            w_counter=0\n",
    "            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "                target_weight = target_weight * (1-TAU) + q_weight * TAU\n",
    "                target_model_theta[w_counter] = target_weight\n",
    "                w_counter+=1\n",
    "            TargetNetwork.set_weights(target_model_theta)\n",
    "        \n",
    "        # update state\n",
    "        state = next_state\n",
    "        score=score+1\n",
    "        if done:                   \n",
    "            print(\"episode: {}, score: {}\".format(it, score))\n",
    "    scores.append(score)\n",
    "TargetNetwork.save('DQNPoleBatchTargetSoftUpdatetanh')\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = tf.keras.models.load_model('DQNPoleBatchTargetSoftUpdatetanh')\n",
    "for i in range(5):\n",
    "    watch_agent(Model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
